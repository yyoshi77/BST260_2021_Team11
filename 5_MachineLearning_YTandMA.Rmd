---
title: "Machine Learning Section"
author: "Yi-Ting Tsai and Mariko Ando (Group 11)"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
load("data.RData")
```

```{r, include=FALSE}
library(tidyverse)
library(dplyr)
library(cutpointr)
library(randomForest)
library(pROC)
library(rpart)
library(caret)
library(splitstackshape)
library(broom)
library(e1071)
library(knitr)
```

## Classification

Aside from looking at the difference between Rio 2016 and Tokyo 2020 Olympics, we are also interested in seeing how we can use the information of COVID cases per population and other covariates to help us "predict" marathon records. Since COVID data is only available for the Tokyo 2020 Olympics, this part of the analysis will focus only on Tokyo 2020 Olympics data. Instead of doing prediction to get the exact performance, we can simplify the task to doing classification. The first thing we have to do is to convert the continuous marathon record data into a binary variable. We can first decide a cutpoint, and then classify the marathon records that are bigger than this point into a “slower” group, and the records smaller than this point into a “faster” group. 

## Cutpoint Analysis for Classification (Yi-Ting Tsai)

Since we are splitting the records into faster and slower group, and men are usually faster, there will be some problem if we combine men’s and women’s data together in this analysis. For example, the faster group are all men, and the slower group are all women. To avoid this potential issue, we will conduct our classification analysis separately on men and women. 

We utilize the training data to determine the optimal cutpoint by the R package `cutpointr`. We first create a label that assigns the top half ranking athletes as 1, and the second half ranking athletes as 0, and use this label to calculate the sensitivity and specificity for different cutpoints. We then choose the cutpoint that gives us the highest sum of sensitivity and specificity.

### Optimal Cutpoint for Men

```{r}
datsecond <- dat %>% 
  filter(dnf==0 & olympic=="Tokyo2020" & sex=="Men") %>%
  dplyr::select(rank, time_sec, case_pp, continent, age, gdp2020, prior_attend) 

datsecond <- datsecond %>% 
  mutate(rank_b = ifelse(as.numeric(rank)<=38, 1, 0))

set.seed(1)
train.rows <- sample(rownames(datsecond), dim(datsecond)[1]*0.7)
test.rows <- setdiff(rownames(datsecond), train.rows)
train_set <- datsecond[train.rows, ]
test_set <- datsecond[test.rows, ]

opt_cut <- cutpointr(train_set, time_sec, rank_b, pos_class = 1, direction = "<=")  #8239
plot_metric(opt_cut)
```

### Optimal Cutpoint for Women

```{r}
datsecond <- dat %>% 
  filter(dnf==0 & olympic=="Tokyo2020" & sex=="Women") %>%
  dplyr::select(rank, time_sec, case_pp, continent, age, gdp2020, prior_attend) 

datsecond <- datsecond %>% 
  mutate(rank_b = ifelse(as.numeric(rank)<=37, 1, 0))

set.seed(1)
train.rows <- sample(rownames(datsecond), dim(datsecond)[1]*0.7)
test.rows <- setdiff(rownames(datsecond), train.rows)
train_set <- datsecond[train.rows, ]
test_set <- datsecond[test.rows, ]

opt_cut <- cutpointr(train_set, time_sec, rank_b, pos_class = 1, direction = "<=")  # 9339
plot_metric(opt_cut)
```

The above two graphs are plotting the sum of sensitivity and specificity (using the function `sum_sens_spec`) against different cutpoints, and we are choosing the cutpoint that gives us the highest value on the y-axis. From this analysis, the optimal cutpoint for men is 8239, and the optimal cutpoint for women is 9339. We will use these cutpoints throughout our following machine learning and bootstrapping models.





## Machine learning: model building & comparison (Mariko Ando)
After we investigated  the athletes' performance by comparing 2016 vs 2020, we also got interested in building models that could classify the athletes' performance during the COVID-19 pandemic by using 2020 Olympic data. By using the best model, we can classify the athletes' performance in the future Olympic Games during the COVID-19 pandemic, which is interesting to athletes and some others. We included the COVID-19 severity, continent where the athletes came from, age, gdp in 2020 of the country where the athletes came from, and prior attendance at Rio 2016 in the model for classifying the performance (1: worse record, 0: better record). As male athletes ran much faster than female athletes, we decided to separately build models for males and females.

We compared six models including logistic regression, Naive Bayes, knn, QDA, LDA, and Trees in terms of model accuracy and discrimination (by AUC). Table 1 shows the summary of model comparison for males, and table 2 shows the summary of model comparison for females.
.

## Analysis for Men's records
```{r, warning=FALSE}
dat=as.data.frame(dat)
```

```{r}
# library
library(tidyverse)
library(rvest)
library(lubridate)
library(broom)
library(caret)
library(ggplot2)
library(e1071)
library(MASS)
library(rpart)
library(randomForest)
library(pROC)
library(adabag)
library(splitstackshape)
library(knitr)
```

```{r}
# filter: 2020, Men, finish race
# I created a dataset, datsecond, just including male athletes who finished race in the Tokyo 2020 game.
# As men ran much faster than women, we separately analyzed the male and female datasets.
datsecond <- dat %>% filter(dnf==0 & olympic=="Tokyo2020" & sex=="Men") %>% dplyr::select(time_sec, case_pp, continent, age, gdp2020, prior_attend) 
summary(datsecond)
```

```{r}
# convert binary and categorical data as factor
datsecond <- datsecond %>% 
  mutate(continent=as.factor(continent), prior_attend=as.factor(prior_attend))

# omit rows with NA
# Here, we decided to conduct complete case analysis.
datsecond<-datsecond %>% filter(!is.na(time_sec))%>% filter(!is.na(case_pp))%>% filter(!is.na(continent))%>% filter(!is.na(age))%>% filter(!is.na(gdp2020))%>% filter(!is.na(prior_attend))
summary(datsecond)

# Based on the calculation by Yi-Ting, one member of our team, we used 8239 sec as a cutpoint for male athletes. We defined time_sec<8239 as better record (outcome=0) and time_sec>=8239 as worse record (outcome=1).

cut <- 8239
datsecond<-datsecond%>%
  mutate(timebinary=ifelse(time_sec<cut,0,1)) %>%
  mutate(timebinary=as.factor(timebinary))
summary(datsecond)


# split train (70%), test (30%)
# We used stratified function to split the dataset into training and testing datasets because we wanted to get almost equal distribution of outcome in both datasets. After splitting, the training set included 49 athletes while the testing set included 22 athletes.
set.seed(1)
x <- stratified(datsecond, "timebinary", 0.70, keep.rownames = TRUE)
train_set <- x %>% dplyr::select(-rn)
train_index <- as.numeric(x$rn)
test_set <- datsecond[-train_index,]
dim(train_set)
dim(test_set)
```

```{r}
# Here, we decided to build six models to predict the worse record.
# Six models include logistic regression, Naive Bayes, knn, QDA, LDA, and Trees. The model included COVID-19 severity (continuous) defined as the case number per population, continent (categorical), age (continuous), gdp2020 (continuous), and prior_attend (binary). 
#In each model, we will report accuracy, sensitivity, and specificity.

# Model1: logistic regression
# accuracy = 0.5000, sensitivity = 0.5833, specificity = 0.4000 
glm_fit <- glm(timebinary ~ case_pp + continent + age + gdp2020 + prior_attend, data = train_set, family = "binomial")
p_hat_logit<-predict(glm_fit, newdata=test_set, type="response")
y_hat_logit <- factor(ifelse(p_hat_logit > 0.5, 1, 0))
confusionMatrix(as.factor(y_hat_logit), reference = test_set$timebinary,positive="1")
```
```{r}
# Model2: Naive Bayes
# accuracy = 0.5909, sensitivity = 0.7500, specificity = 0.4000
nb_fit <- naiveBayes(timebinary ~ case_pp + continent + age + gdp2020 + prior_attend, data = train_set)
p_hat_nb<-predict(nb_fit, newdata=test_set, type="raw")[,2]
y_hat_nb<-factor(ifelse(p_hat_nb>0.5,1,0))
confusionMatrix(data=as.factor(y_hat_nb),reference=test_set$timebinary,positive="1")
```
```{r}
# Model3: knn
# For the value of k, we chose the number closest to the squared root of the sample size of the training set, which was 7.
# accuracy =  0.5000, sensitivity = 0.5833, specificity = 0.4000 
knn_fit<-knn3(timebinary ~ case_pp + continent + age  + gdp2020 + prior_attend, data = train_set, k=7)
p_hat_knn<-predict(knn_fit,newdata=test_set)[,2]
y_hat_knn<-factor(ifelse(p_hat_knn>0.5,1,0))
confusionMatrix(data=as.factor(y_hat_knn),reference=test_set$timebinary,positive="1")
```

```{r}
# Model4: QDA 
# accuracy = 0.5000 , sensitivity = 0.6667, specificity = 0.3000
set.seed(1)
qda_fit <- qda(timebinary ~ case_pp + continent + age + gdp2020 + prior_attend, data = train_set)
p_hat_qda <- predict(qda_fit,newdata=test_set)$posterior[,2]
y_hat_qda <- factor(ifelse(p_hat_qda>0.5,1,0))
confusionMatrix(data=as.factor(y_hat_qda),reference=test_set$timebinary,positive="1")
```
```{r}
# Model5: LDA
# accuracy = 0.5000, sensitivity = 0.5833, specificity = 0.4000
set.seed(1)
lda_fit <- lda(timebinary ~ case_pp + continent + age + gdp2020 + prior_attend, data = train_set)
p_hat_lda <- predict(lda_fit,newdata=test_set)$posterior[,2]
y_hat_lda <- factor(ifelse(p_hat_lda>0.5,1,0))
confusionMatrix(data=as.factor(y_hat_lda),reference=test_set$timebinary,positive="1")
```
```{r}
# Model6: Decision trees
# accuracy = 0.5455, sensitivity = 0.5833, specificity = 0.5000
set.seed(1)
tree_fit <- rpart(timebinary ~ case_pp + continent + age + gdp2020 + prior_attend, data = train_set)
p_hat_tree <- predict(tree_fit,newdata=test_set)[,2]
y_hat_tree <- factor(ifelse(p_hat_tree>0.5,1,0))
confusionMatrix(data=as.factor(y_hat_tree),reference=test_set$timebinary,positive="1")
```
```{r}
# plot ROC curve
# Here, we compare ROC curves and AUC to see which model has a better discrimination.

roc_logit<-roc(test_set$timebinary,p_hat_logit)
roc_nb<-roc(test_set$timebinary,p_hat_nb)
roc_knn<-roc(test_set$timebinary,p_hat_knn)
roc_qda<-roc(test_set$timebinary,p_hat_qda)
roc_lda<-roc(test_set$timebinary,p_hat_lda)
roc_tree<-roc(test_set$timebinary,p_hat_tree)
ggroc(list("Logistic regression"=roc_logit,"Naive Bayes"=roc_nb,"kNN, k=7"=roc_knn, "QDA model"=roc_qda,"LDA model"=roc_lda, "Decision Tree"=roc_tree))+
  theme(legend.title=element_blank())+
  geom_segment(aes(x=1, xend=0,y=0,yend=1),color="black",linetype="dotted")+
  ggtitle("ROC curves") +
  xlab("Specificity")+
  ylab("Sensitivity")+ 
  theme(plot.title = element_text(hjust = 0.5))

#Naive Bayes model has the highest AUC.
auc(roc_logit)
auc(roc_nb)
auc(roc_knn)
auc(roc_qda)
auc(roc_lda)
auc(roc_tree)
```

```{r}
#Table for men's models
Men_models <- c("Logistic", "Naive Bayes", "kNN, k=7", "QDA", "LDA", "Trees")
Accuracy <- c("0.5000", "0.5909", "0.5000", "0.5000", "0.5000", "0.5455")
Sensitivity <- c("0.5833", "0.7500", "0.5833", "0.6667", "0.5833", "0.5833")
Specificity <- c("0.4000", "0.4000", "0.4000", "0.3000", "0.4000", "0.5000")
PPV <- c("0.5385", "0.6000", "0.5385", "0.5333", "0.5385", "0.5833")
NPV <- c("0.4444", "0.5714", "0.4444", "0.4286", "0.4444", "0.5000")
AUC <- c("0.4917", "0.6500", "0.4833", "0.5375", "0.5333", "0.5625")
male_df <- data.frame(Men_models, Accuracy, Sensitivity, Specificity, PPV, NPV, AUC)

male_df2 <- head(male_df)
knitr::kable(male_df2, col.names = gsub("[.]", " ", names(male_df)), caption = "Table 1: Model comparison for male athletes")
```
##### (Summary) According to the outputs, Naive Bayes model showed the highest accuracy and the highest AUC (i.e., best discrimination). Then, Naive Bayes model could be the best model to classify male athletes' performance in the future Olympic Games during a similar pandemic. However, as the current dataset included very small number of samples, we should ideally repeat similar analyses by using bootstrapping methods or other datasets with larger sample size of athletes and compare the models. 


## Analysis for women's records
```{r}
# filter: 2020, Women, finish race
# I created a dataset, datsecondfemale, just including female athletes who finished race in the Tokyo 2020 game.

datsecondfemale <- dat %>% filter(dnf==0 & olympic=="Tokyo2020" & sex=="Women") %>% dplyr::select(time_sec, case_pp, continent, age, gdp2020, prior_attend) 
summary(datsecondfemale)

# convert binary and categorical data as factor
datsecondfemale <- datsecondfemale %>% 
  mutate(continent=as.factor(continent), prior_attend=as.factor(prior_attend))

# omit rows with NA
#Here, we decided to conduct complete case analysis again.
datsecondfemale<-datsecondfemale %>% filter(!is.na(time_sec))%>% filter(!is.na(case_pp))%>% filter(!is.na(continent))%>% filter(!is.na(age))%>% filter(!is.na(gdp2020))%>% filter(!is.na(prior_attend))
summary(datsecondfemale)
```


```{r}
# Based on the calculation by Yi-Ting, one member of our team, we used 9339 sec as a cutpoint for female athletes. We defined time_sec<9339 as better record (outcome=0) and time_sec>=9339 as worse record (outcome=1).

cut_female <- 9339
datsecondfemale<-datsecondfemale%>%
   mutate(timebinary=ifelse(time_sec<cut_female,0,1)) %>%
   mutate(timebinary=as.factor(timebinary))
summary(datsecondfemale)


# split train (70%), test (30%)
# We used stratified function to split the dataset into training and testing datasets because we wanted to get almost equal distribution of outcome in both datasets. After splitting, the training set included 49 athletes while the testing set included 22 athletes.
set.seed(1)
x_female <- stratified(datsecondfemale, "timebinary", 0.70, keep.rownames = TRUE)
train_set_female <- x_female %>% dplyr::select(-rn)
train_index_female <- as.numeric(x_female$rn)
test_set_female <- datsecondfemale[-train_index_female,]
dim(train_set_female)
dim(test_set_female)
```

```{r}
# Here, we decided to build six models to predict the worse record.
# Six models include logistic regression, Naive Bayes, knn, QDA, LDA, and Trees. The model included COVID-19 severity (continuous) defined as the case number per population, continent (categorical), age (continuous), gdp2020 (continuous), and prior_attend (binary).  
#In each model, we will report accuracy, sensitivity, and specificity.

# Model1: logistic regression
# accuracy = 0.5909, sensitivity = 0.7273 , specificity = 0.4545 
glm_fit_female <- glm(timebinary ~ case_pp + continent + age + gdp2020 + prior_attend, data = train_set_female, family = "binomial")
p_hat_logit_female<-predict(glm_fit_female, newdata=test_set_female, type="response")
y_hat_logit_female <- factor(ifelse(p_hat_logit_female > 0.5, 1, 0))
confusionMatrix(as.factor(y_hat_logit_female), reference = test_set_female$timebinary,positive="1")
```
```{r}
# Model2: Naive Bayes
# accuracy = 0.3636, sensitivity = 0.4545, specificity = 0.2727  
nb_fit_female <- naiveBayes(timebinary ~ case_pp + continent + age + gdp2020 + prior_attend, data = train_set_female)
p_hat_nb_female<-predict(nb_fit, newdata=test_set_female, type="raw")[,2]
y_hat_nb_female<-factor(ifelse(p_hat_nb_female>0.5,1,0))
confusionMatrix(data=as.factor(y_hat_nb_female),reference=test_set_female$timebinary,positive="1")
```

```{r}
# Model3: knn
# For the value of k, I chose the number closest to the squared root of the sample size of the training set, which was 7.
# accuracy = 0.7273, sensitivity = 0.6364, specificity = 0.8182
knn_fit_female<-knn3(timebinary ~ case_pp + continent + age  + gdp2020 + prior_attend, data = train_set_female, k=7)
p_hat_knn_female<-predict(knn_fit_female,newdata=test_set_female)[,2]
y_hat_knn_female<-factor(ifelse(p_hat_knn_female>0.5,1,0))
confusionMatrix(data=as.factor(y_hat_knn_female),reference=test_set_female$timebinary,positive="1")
```


```{r}
# Model4: QDA  
# accuracy = 0.4091 , sensitivity = 0.4545, specificity = 0.3636
set.seed(1)
qda_fit_female <- qda(timebinary ~ case_pp + continent + age + gdp2020 + prior_attend, data = train_set_female)
p_hat_qda_female <- predict(qda_fit_female,newdata=test_set_female)$posterior[,2]
y_hat_qda_female <- factor(ifelse(p_hat_qda_female>0.5,1,0))
confusionMatrix(data=as.factor(y_hat_qda_female),reference=test_set_female$timebinary,positive="1")
```

```{r}
# Model5: LDA
# accuracy = 0.5455 , sensitivity = 0.7273, specificity = 0.3636
set.seed(1)
lda_fit_female <- lda(timebinary ~ case_pp + continent + age + gdp2020 + prior_attend, data = train_set_female)
p_hat_lda_female <- predict(lda_fit_female,newdata=test_set_female)$posterior[,2]
y_hat_lda_female <- factor(ifelse(p_hat_lda_female>0.5,1,0))
confusionMatrix(data=as.factor(y_hat_lda_female),reference=test_set_female$timebinary,positive="1")
```

```{r}
# Model6: Decision trees
# accuracy = 0.5909, sensitivity =  0.6364, specificity = 0.5455  
set.seed(1)
tree_fit_female <- rpart(timebinary ~ case_pp + continent + age + gdp2020 + prior_attend, data = train_set_female)
p_hat_tree_female <- predict(tree_fit_female,newdata=test_set_female)[,2]
y_hat_tree_female <- factor(ifelse(p_hat_tree_female>0.5,1,0))
confusionMatrix(data=as.factor(y_hat_tree_female),reference=test_set_female$timebinary,positive="1")
```


```{r}
# plot ROC curve
# Here, we compare ROC curves and AUC to see which model has a better discrimination.
roc_logit_female<-roc(test_set_female$timebinary,p_hat_logit_female)
roc_nb_female<-roc(test_set_female$timebinary,p_hat_nb_female)
roc_knn_female<-roc(test_set_female$timebinary,p_hat_knn_female)
roc_qda_female<-roc(test_set_female$timebinary,p_hat_qda_female)
roc_lda_female<-roc(test_set_female$timebinary,p_hat_lda_female)
roc_tree_female<-roc(test_set_female$timebinary,p_hat_tree_female)
ggroc(list("Logistic regression"=roc_logit_female,"Naive Bayes"=roc_nb_female,"kNN, k=7"=roc_knn_female, "QDA model"=roc_qda_female, "LDA model"=roc_lda_female, "Decision Tree"=roc_tree_female))+
  theme(legend.title=element_blank())+
  geom_segment(aes(x=1, xend=0,y=0,yend=1),color="black",linetype="dotted")+
  ggtitle("ROC curves") +
  xlab("Specificity")+
  ylab("Sensitivity")+ 
  theme(plot.title = element_text(hjust = 0.5))

#Decision Trees is the best model in terms of AUC in women.
auc(roc_logit_female)
auc(roc_nb_female)
auc(roc_knn_female)
auc(roc_qda_female)
auc(roc_lda_female)
auc(roc_tree_female)
```

```{r}
#Table for women's models
Women_models <- c("Logistic", "Naive Bayes", "kNN, k=7", "QDA", "LDA", "Trees")
Accuracy <- c("0.5909", "0.3636", "0.7273", "0.4091", "0.5455", "0.5909")
Sensitivity <- c("0.7273", "0.4545", "0.6364", "0.4545", "0.7273", "0.6364")
Specificity <- c("0.4545", "0.2727", "0.8182", "0.3636", "0.3636", "0.5455")
PPV <- c("0.5714", "0.3846", "0.7778", "0.4167", "0.5333", "0.5833")
NPV <- c("0.6250", "0.3333", "0.6923", "0.4000", "0.5714", "0.6000")
AUC <- c("0.5868", "0.7107", "0.8017", "0.5868", "0.5702", "0.5826")
female_df <- data.frame(Women_models, Accuracy, Sensitivity, Specificity, PPV, NPV, AUC)

female_df2 <- head(female_df)
knitr::kable(female_df2, col.names = gsub("[.]", " ", names(female_df)), caption = "Table 2: Model comparison for female athletes")
```

##### (Summary) According to the outputs, the knn model showed the highest accuracy and the highest AUC (i.e., best discrimination). Then, knn model could be the best model to classify female athletes’ performance in the future Olympic Games during a similar pandemic. However, as the current dataset included very small number of samples, we should ideally repeat similar analyses by using bootstrapping methods or other datasets with larger sample size of athletes and compare the models.









## Bootstrapping (Yi-Ting Tsai)

Since the sample size of our dataset is very small, only 71 observations for men and women, respectively, this may cause a problem when training machine learning models. Therefore, we try to apply some methods that incorporate bootstrapping to see whether we can get better results. 

We try to compare two sets of original vs. bootstrapped models:

1. Decision tree and random forest

2. Naive Bayes and the bootstrapped version of Naive Bayes

We choose to compare decision tree and random forest since random forest is a very well-known machine learning model, and we would really like to see how can bootstrapping makes a difference between these two models. We choose Naive Bayes since it performs well among all of the machine learning models we've tried above (the best for men and the second best for women if we are looking at AUC). We would like to know whether doing bootstrapping can make the performances become even better.

For random forest, we simply apply the random forest function in R. For Naive Bayes, we do the bootstrap manually. We first sample with replacement from the training data 100 times, each time with sample size equal to the training set. Then, for each resample, we fit a model and get the predicted class for each test data. We then use majority vote among these 100 models to determine the class of each data. 

We will first fit the models and then present the ROC curves and some statistics at the end to do some comparisons.

## Bootstrapping for Men

```{r}
# preparing the data
# filter: 2020, Men, finish race
datsecond <- dat %>% 
  filter(dnf==0 & olympic=="Tokyo2020" & sex=="Men") %>%
  dplyr::select(time_sec, case_pp, continent, age, gdp2020, prior_attend) 

# convert categorical/binary variables into factors
datsecond <- datsecond %>% 
  mutate(continent=as.factor(continent), prior_attend=as.factor(prior_attend))

# omit the rows with NA
datsecond<-datsecond %>% filter(!is.na(time_sec))%>% filter(!is.na(case_pp))%>% filter(!is.na(continent))%>% filter(!is.na(age))%>% filter(!is.na(gdp2020))%>% filter(!is.na(prior_attend))

# add the binary outcome variable
cut <- 8239
datsecond<-datsecond%>%
  mutate(timebinary=ifelse(time_sec<cut,0,1)) %>%
  mutate(timebinary=as.factor(timebinary))

# split train (70%), test (30%)
set.seed(1)
x <- stratified(datsecond, "timebinary", 0.70, keep.rownames = TRUE)
train_set <- x %>% dplyr::select(-rn)
train_index <- as.numeric(x$rn)
test_set <- datsecond[-train_index,]
```


### Decision Tree and Random Forest (Men)

```{r}
# Decision trees
set.seed(1)
tree_fit <- rpart(timebinary ~ case_pp + continent + age + gdp2020 + prior_attend, data = train_set)
p_hat_tree <- predict(tree_fit,newdata=test_set)[,2]
y_hat_tree <- factor(ifelse(p_hat_tree>0.5,1,0))
confusionMatrix(data=as.factor(y_hat_tree),reference=test_set$timebinary,positive="1")

# Random Forest
set.seed(1)
rf_fit <- randomForest(timebinary ~ case_pp + continent + age + gdp2020 + prior_attend, data = train_set)
p_hat_rf <- predict(rf_fit,newdata=test_set,type="prob")[,2]
y_hat_rf <- factor(ifelse(p_hat_rf>0.5,1,0))
confusionMatrix(data=as.factor(y_hat_rf),reference=test_set$timebinary,positive="1")

# ROC curves
roc_tree <- pROC::roc(test_set$timebinary,p_hat_tree)
roc_rf <- pROC::roc(test_set$timebinary,p_hat_rf)
p1 <- ggroc(list("Decision Tree"=roc_tree, "Random Forest"=roc_rf))+
  theme(legend.title=element_blank())+
  geom_segment(aes(x=1, xend=0,y=0,yend=1),color="black",linetype="dotted")+
  ggtitle("ROC curves - tree (men)") +
  xlab("Specificity")+ylab("Sensitivity")+
  theme(plot.title = element_text(hjust = 0.5))

## AUC
pROC::auc(roc_tree)
pROC::auc(roc_rf)
```

### Naive Bayes and the Bootstrapped Version (Men)
```{r}
# Naive Bayes
nb_fit <- naiveBayes(timebinary ~ case_pp + continent + age + gdp2020 + prior_attend, data = train_set)
p_hat_nb<-predict(nb_fit, newdata=test_set, type="raw")[,2]
y_hat_nb<-factor(ifelse(p_hat_nb>0.5,1,0))
confusionMatrix(data=as.factor(y_hat_nb),reference=test_set$timebinary,positive="1")

# bootstrapped version of naive bayes
rows <- 1:49
pred_class_table <- data.frame(matrix(ncol = nrow(test_set), nrow = 0))

for(iter in 1:100){
  # create the resample train set
  sample_row <- sample(rows, size=85, replace = TRUE)
  resample_train_set <- data.frame()
  for(i in sample_row){
    add_this <- train_set[i,]
    resample_train_set <- rbind(resample_train_set, add_this)
  }
  nb_fit<-naiveBayes(timebinary ~ case_pp + continent + age + gdp2020 + prior_attend, data = resample_train_set)
  p_hat_nb_b<-predict(nb_fit,newdata=test_set,type="raw")[,2]
  y_hat_nb_b<-ifelse(p_hat_nb_b>0.5,1,0)
  pred_class_table <- rbind(pred_class_table, y_hat_nb_b)
}

p_hat_nb_b <- apply(pred_class_table,2,sum)/100
y_hat_nb_b <- factor(ifelse(p_hat_nb_b > 0.5, 1, 0))

confusionMatrix(as.factor(y_hat_nb_b), reference = test_set$timebinary,positive="1")

# ROC curves
roc_nb <- pROC::roc(test_set$timebinary,p_hat_nb)
roc_nb_b <- pROC::roc(test_set$timebinary,p_hat_nb_b)
p2 <- ggroc(list("nb"=roc_nb, "nb (bootstrapped)"=roc_nb_b))+
  theme(legend.title=element_blank())+
  geom_segment(aes(x=1, xend=0,y=0,yend=1),color="black",linetype="dotted")+
  ggtitle("ROC curves - Naive Bayes (men)") +
  xlab("Specificity")+ylab("Sensitivity")+
  theme(plot.title = element_text(hjust = 0.5))

## AUC
pROC::auc(roc_nb)
pROC::auc(roc_nb_b)
```


## Bootstrapping for Women

```{r}
# preparing the data
# filter: 2020, Women, finish race
datsecond <- dat %>% 
  filter(dnf==0 & olympic=="Tokyo2020" & sex=="Women") %>%
  dplyr::select(time_sec, case_pp, continent, age, gdp2020, prior_attend) 

# convert categorical/binary variables into factors
datsecond <- datsecond %>% 
  mutate(continent=as.factor(continent), prior_attend=as.factor(prior_attend))

# omit the rows with NA
datsecond<-datsecond %>% filter(!is.na(time_sec))%>% filter(!is.na(case_pp))%>% filter(!is.na(continent))%>% filter(!is.na(age))%>% filter(!is.na(gdp2020))%>% filter(!is.na(prior_attend))

# add the binary outcome variable
cut <- 9339
datsecond<-datsecond%>%
  mutate(timebinary=ifelse(time_sec<cut,0,1)) %>%
  mutate(timebinary=as.factor(timebinary))

# split train (70%), test (30%)
set.seed(1)
x <- stratified(datsecond, "timebinary", 0.70, keep.rownames = TRUE)
train_set <- x %>% dplyr::select(-rn)
train_index <- as.numeric(x$rn)
test_set <- datsecond[-train_index,]
```


### Decision Tree and Random Forest (Women)

```{r}
# Decision trees
set.seed(1)
tree_fit <- rpart(timebinary ~ case_pp + continent + age + gdp2020 + prior_attend, data = train_set)
p_hat_tree <- predict(tree_fit,newdata=test_set)[,2]
y_hat_tree <- factor(ifelse(p_hat_tree>0.5,1,0))
confusionMatrix(data=as.factor(y_hat_tree),reference=test_set$timebinary,positive="1")

# Random Forest
set.seed(1)
rf_fit <- randomForest(timebinary ~ case_pp + continent + age + gdp2020 + prior_attend, data = train_set)
p_hat_rf <- predict(rf_fit,newdata=test_set,type="prob")[,2]
y_hat_rf <- factor(ifelse(p_hat_rf>0.5,1,0))
confusionMatrix(data=as.factor(y_hat_rf),reference=test_set$timebinary,positive="1")

# ROC curves
roc_tree <- pROC::roc(test_set$timebinary,p_hat_tree)
roc_rf <- pROC::roc(test_set$timebinary,p_hat_rf)
p3 <- ggroc(list("Decision Tree"=roc_tree, "Random Forest"=roc_rf))+
  theme(legend.title=element_blank())+
  geom_segment(aes(x=1, xend=0,y=0,yend=1),color="black",linetype="dotted")+
  ggtitle("ROC curves - tree (women)") +
  xlab("Specificity")+ylab("Sensitivity")+
  theme(plot.title = element_text(hjust = 0.5))

## AUC
pROC::auc(roc_tree)
pROC::auc(roc_rf)
```

### Naive Bayes and the Bootstrapped Version (Women)
```{r}
# Naive Bayes
nb_fit <- naiveBayes(timebinary ~ case_pp + continent + age + gdp2020 + prior_attend, data = train_set)
p_hat_nb<-predict(nb_fit, newdata=test_set, type="raw")[,2]
y_hat_nb<-factor(ifelse(p_hat_nb>0.5,1,0))
confusionMatrix(data=as.factor(y_hat_nb),reference=test_set$timebinary,positive="1")

# bootstrapped version of Naive Bayes
rows <- 1:49
pred_class_table <- data.frame(matrix(ncol = nrow(test_set), nrow = 0))

for(iter in 1:100){
  # create the resample train set
  sample_row <- sample(rows, size=85, replace = TRUE)
  resample_train_set <- data.frame()
  for(i in sample_row){
    add_this <- train_set[i,]
    resample_train_set <- rbind(resample_train_set, add_this)
  }
  nb_fit<-naiveBayes(timebinary ~ case_pp + continent + age + gdp2020 + prior_attend, data = resample_train_set)
  p_hat_nb_b<-predict(nb_fit,newdata=test_set,type="raw")[,2]
  y_hat_nb_b<-ifelse(p_hat_nb_b>0.5,1,0)
  pred_class_table <- rbind(pred_class_table, y_hat_nb_b)
}

p_hat_nb_b <- apply(pred_class_table,2,sum)/100
y_hat_nb_b <- factor(ifelse(p_hat_nb_b > 0.5, 1, 0))

confusionMatrix(as.factor(y_hat_nb_b), reference = test_set$timebinary,positive="1")

# ROC curves
roc_nb <- pROC::roc(test_set$timebinary,p_hat_nb)
roc_nb_b <- pROC::roc(test_set$timebinary,p_hat_nb_b)
p4 <- ggroc(list("nb"=roc_nb, "nb (bootstrapped)"=roc_nb_b))+
  theme(legend.title=element_blank())+
  geom_segment(aes(x=1, xend=0,y=0,yend=1),color="black",linetype="dotted")+
  ggtitle("ROC curves - Naive Bayes (women)") +
  xlab("Specificity")+ylab("Sensitivity")+
  theme(plot.title = element_text(hjust = 0.5))

## AUC
pROC::auc(roc_nb)
pROC::auc(roc_nb_b)
```

### ROC Curves

The ROC curves comparing "decision tree and random forest", "Naive Bayes and the bootstrapped version of Naive Bayes", for men and women, are as follows. 

In the case of "decision tree and random forest" for women, we can see that the ROC curve moves towards the upper left corner after bootstrapping. However, in the other three cases, the ROC curves do not seem to be very different before and after bootstrapping.


```{r}
p1
p2
p3
p4
```


### Summary Statistics

Here, we list out some summary statistics from the confusion matrix, for men and women, respectively. 

```{r}
# Table for men's models
Men_models <- c("Tree", "Random Forest", "NB", "NB (bootstrapped)")
Accuracy <- c("0.5455", "0.5455", "0.5909", "0.5455")
Sensitivity <- c("0.5833", "0.6667", "0.7500", "0.6667")
Specificity <- c("0.5000", "0.4000", "0.4000", "0.4000")
PPV <- c("0.5833", "0.5714", "0.6000", "0.5714")
NPV <- c("0.5000", "0.5000", "0.5714", "0.5000")
AUC <- c("0.5625", "0.5458", "0.65", "0.6167")

male_df <- data.frame(Men_models, Accuracy, Sensitivity, Specificity, PPV, NPV, AUC)

kable(male_df, format = "markdown", digits = 2)

# Table for men's models
Women_models <- c("Tree", "Random Forest", "NB", "NB (bootstrapped)")
Accuracy <- c("0.5909", "0.5909", "0.4545", "0.4545")
Sensitivity <- c("0.6364", "0.5455", "0.5455", "0.5455")
Specificity <- c("0.5455", "0.6364", "0.3636", "0.3636")
PPV <- c("0.5833", "0.6000", "0.4615", "0.4615")
NPV <- c("0.6000", "0.5833", "0.4444", "0.4444")
AUC <- c("0.5826", "0.6777", "0.5207", "0.4793")

female_df <- data.frame(Women_models, Accuracy, Sensitivity, Specificity, PPV, NPV, AUC)

kable(female_df, format = "markdown", digits = 2)
```

### Discussion and Comparison

We can now compare the result of decision tree to random forest, and Naive Bayes to the bootstrapped version of Naive Bayes. If we focus on AUC, we discovered that bootstrapping is only helpful in one of the four comparisons: the women case of tree and random forest. The AUC increases from 0.5826 to 0.6777. For the other 3 cases, bootstrapping either doesn't have effect on the performance accuracy or is making the result even less accurate. 

This result is quite different from our expectation, since we are hoping that bootstrapping can improve the performance of our classification models a bit. Perhaps this is still due to our small sample size problem, and it may be better if we could have a larger dataset.

